0. 输入：第一层，障碍物地图，有障碍物为1，无障碍物为0；第二层，目的地点地图，有目的地值为10，无目的地值为0；
1. 障碍物地图和目的地地图经过2次卷积，得到了奖赏R；
2. 奖赏R经过 1次卷积 得到Q初始值；
3. 给Q初始值取最大值得到V；
4. 把R和V拼起来作为输入，再经过 那1次卷积 的参数和初始化的参数
拼成的新卷积网络P，得到新一轮Q；（这里实际上是做了RNN，但是
代码没采用现成RNN，而是自己用循环实现了）
5. 给新Q去最大值得到新V；
以上两步循环做k-1次RNN；
6. 做最后1次RNN；
7. 经过一个全连接层fc；
8. 将fc的8输出做softmax；
9. 将网络输出的动作序列结合当前状态就能得到路径；
10. 将得到路径与A*产生路径作比较，得出训练误差；（这就是模仿学习）
